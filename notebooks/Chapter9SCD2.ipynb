{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d716b355",
   "metadata": {},
   "source": [
    "## Implementing SCD-2 with Apache Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73dcb796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_REGION=region\n",
      "env: AWS_ACCESS_KEY_ID=key\n",
      "env: AWS_SECRET_ACCESS_KEY=secret\n"
     ]
    }
   ],
   "source": [
    "# Define the AWS env variables if you are using AWS Auth:\n",
    "%env AWS_REGION= region\n",
    "%env AWS_ACCESS_KEY_ID= key\n",
    "%env AWS_SECRET_ACCESS_KEY= secret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb8f3e",
   "metadata": {},
   "source": [
    "# Define configurations for Spark, Iceberg & Catalog (Glue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c545e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/docker/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/docker/.ivy2/cache\n",
      "The jars for the packages stored in: /home/docker/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c543c82c-5cdb-445a-a9af-9ad92e17b8d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.2.0 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.178 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#utils;2.17.178 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.17.178 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.17.178 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.17.178 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 4278ms :: artifacts dl 391ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.2.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.17.178 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c543c82c-5cdb-445a-a9af-9ad92e17b8d5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/79ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/15 17:03:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/15 17:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/15 17:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/15 17:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/09/15 17:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/09/15 17:03:56 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "Spark Running\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('app_name')\n",
    "    \n",
    "    # first we will define the packages that we need. Iceberg Spark runtime\n",
    "        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.2.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178,org.apache.hadoop:hadoop-aws:3.3.1')\n",
    "        \n",
    "    # This property allows us to add any extensions that we want to use\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "    \n",
    "    # configures a new catalog to a particular implementation of SparkCatalog\n",
    "        .set('spark.sql.catalog.glue', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "    \n",
    "    # particular type of catalog we are using\n",
    "        .set('spark.sql.catalog.glue.catalog-impl', 'org.apache.iceberg.aws.glue.GlueCatalog')\n",
    "    \n",
    "    # engine writes to the warehouse\n",
    "        .set('spark.sql.catalog.glue.warehouse', 's3://my-bucket/warehouse/')\n",
    "    \n",
    "    # changes IO impl of catalog, mainly for changing writing data to object storage\n",
    "        .set('spark.sql.catalog.glue.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    ")\n",
    "\n",
    "## Start Spark Session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "print(\"Spark Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb443762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    DateType,\n",
    "    BooleanType,\n",
    "    TimestampType\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305e96c",
   "metadata": {},
   "source": [
    "# Define customer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a652c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_customer_schema = StructType([\n",
    "        StructField('customer_id', StringType(), False),\n",
    "        StructField('first_name', StringType(), True),\n",
    "        StructField('last_name', StringType(), True),\n",
    "        StructField('city', StringType(), True),\n",
    "        StructField('country', StringType(), True),\n",
    "        StructField('eff_start_date', DateType(), True),\n",
    "        StructField('eff_end_date', DateType(), True),\n",
    "        StructField('timestamp', TimestampType(), True),\n",
    "        StructField('is_current', BooleanType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d90a08",
   "metadata": {},
   "source": [
    "# Create customer records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ee9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import time\n",
    "\n",
    "random_udf = udf(lambda: str(int(time.time() * 1000000)), StringType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b36a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<jemalloc>: MADV_DONTNEED does not work (memset will be used instead)\n",
      "<jemalloc>: (This is the expected behaviour if you are running under QEMU)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|customer_id|first_name|last_name|city   |country|eff_start_date|eff_end_date|timestamp          |is_current|customer_dim_key|\n",
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|1          |Morgan    |Brown    |Toronto|Canada |2020-09-27    |2999-12-31  |2020-12-08 09:15:32|true      |1694797722535090|\n",
      "|2          |Angie     |Keller   |Chicago|US     |2020-10-14    |2999-12-31  |2020-12-08 09:15:32|true      |1694797722510645|\n",
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "customer_dim_df = spark.createDataFrame([('1', 'Morgan', 'Brown', 'Toronto', 'Canada', datetime.strptime('2020-09-27', '%Y-%m-%d'), datetime.strptime('2999-12-31', '%Y-%m-%d'), datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'), True),\n",
    "                       ('2', 'Angie', 'Keller', 'Chicago', 'US', datetime.strptime('2020-10-14', '%Y-%m-%d'), datetime.strptime('2999-12-31', '%Y-%m-%d'), datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'), True)], dim_customer_schema)\n",
    "\n",
    "customer_ice_df = customer_dim_df.withColumn(\"customer_dim_key\", random_udf())\n",
    "\n",
    "customer_ice_df.cache()\n",
    "\n",
    "customer_ice_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5a272",
   "metadata": {},
   "source": [
    "# Store customer records as an Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37279ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "customer_ice_df.writeTo(\"glue.dip.customers\").partitionedBy(col(\"country\")).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5119910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>eff_start_date</th>\n",
       "      <th>eff_end_date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_current</th>\n",
       "      <th>customer_dim_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2020-09-27</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-08 09:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694797722535090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Keller</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-08 09:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694797722510645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id first_name last_name     city country eff_start_date  \\\n",
       "0           1     Morgan     Brown  Toronto  Canada     2020-09-27   \n",
       "1           2      Angie    Keller  Chicago      US     2020-10-14   \n",
       "\n",
       "  eff_end_date           timestamp  is_current  customer_dim_key  \n",
       "0   2999-12-31 2020-12-08 09:15:32        True  1694797722535090  \n",
       "1   2999-12-31 2020-12-08 09:15:32        True  1694797722510645  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM glue.dip.customers\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fcb62",
   "metadata": {},
   "source": [
    "# Sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "609151a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+-------------------+-----------+\n",
      "|item_id|quantity|price|          timestamp|customer_id|\n",
      "+-------+--------+-----+-------------------+-----------+\n",
      "|    111|      40| 90.5|2020-11-17 09:15:32|          1|\n",
      "|    112|     250|80.65|2020-10-28 09:15:32|          1|\n",
      "|    113|      10|600.5|2020-12-08 09:15:32|          2|\n",
      "+-------+--------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "fact_sales_schema = StructType([\n",
    "        StructField('item_id', StringType(), True),\n",
    "        StructField('quantity', IntegerType(), True),\n",
    "        StructField('price', DoubleType(), True),\n",
    "        StructField('timestamp', TimestampType(), True),\n",
    "        StructField('customer_id', StringType(), True)\n",
    "    ])\n",
    "\n",
    "sales_fact_df = spark.createDataFrame([('111', 40, 90.5, datetime.strptime('2020-11-17 09:15:32', '%Y-%m-%d %H:%M:%S'), '1'),\n",
    "                                       ('112', 250, 80.65, datetime.strptime('2020-10-28 09:15:32', '%Y-%m-%d %H:%M:%S'), '1'),\n",
    "                                      ('113', 10, 600.5, datetime.strptime('2020-12-08 09:15:32', '%Y-%m-%d %H:%M:%S'), '2')], fact_sales_schema)\n",
    "\n",
    "sales_fact_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed648a",
   "metadata": {},
   "source": [
    "# Customer dimension key lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7773be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "join_cond = [sales_fact_df.customer_id == customer_ice_df.customer_id,\n",
    "             sales_fact_df.timestamp >= customer_ice_df.eff_start_date,\n",
    "             sales_fact_df.timestamp < customer_ice_df.eff_end_date]\n",
    "\n",
    "customers_dim_key_df = (sales_fact_df\n",
    "                          .join(customer_ice_df, join_cond, 'leftouter')\n",
    "                          .select(sales_fact_df['*'],\n",
    "                            when(customer_ice_df.customer_dim_key.isNull(), '-1')\n",
    "                                  .otherwise(customer_ice_df.customer_dim_key)\n",
    "                                  .alias(\"customer_dim_key\") )\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610f7424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "|item_id|quantity|price|          timestamp|customer_id|customer_dim_key|\n",
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "|    111|      40| 90.5|2020-11-17 09:15:32|          1|1694797722535090|\n",
      "|    112|     250|80.65|2020-10-28 09:15:32|          1|1694797722535090|\n",
      "|    113|      10|600.5|2020-12-08 09:15:32|          2|1694797722510645|\n",
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_dim_key_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610feb0f",
   "metadata": {},
   "source": [
    "# Save sales data as another Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698cda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_dim_key_df.writeTo(\"glue.dip.sales2\").create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d8c2b5",
   "metadata": {},
   "source": [
    "# Read sales table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5dde51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "|item_id|quantity|price|          timestamp|customer_id|customer_dim_key|\n",
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "|    111|      40| 90.5|2020-11-17 09:15:32|          1|1694797722535090|\n",
      "|    112|     250|80.65|2020-10-28 09:15:32|          1|1694797722535090|\n",
      "|    113|      10|600.5|2020-12-08 09:15:32|          2|1694797722510645|\n",
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * from glue.dip.sales2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919edc1",
   "metadata": {},
   "source": [
    "# Get number of sales per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "244222f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------+\n",
      "|country|sales_quantity|count_sales|\n",
      "+-------+--------------+-----------+\n",
      "| Canada|           290|          2|\n",
      "|     US|            10|          1|\n",
      "+-------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    'SELECT ct.country, '\n",
    "    'SUM(st.quantity) as sales_quantity,'\n",
    "    'COUNT(*) as count_sales '\n",
    "    'FROM glue.dip.sales2 st '\n",
    "    'INNER JOIN glue.dip.customers ct on st.customer_dim_key = ct.customer_dim_key '\n",
    "    'group by ct.country').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430c9a77",
   "metadata": {},
   "source": [
    "# Customer Angie changed the country from US to FR, new customer Sebastian created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4480e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|customer_id|first_name|last_name| city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n",
      "+-----------+----------+---------+-----+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|          3| Sebastian|    White| Rome|     IT|    2023-09-15|  2999-12-31|2020-12-09 09:15:32|      true|1694798225942495|\n",
      "|          2|     Angie|   Keller|Paris|     FR|    2023-09-15|  2999-12-31|2020-12-09 10:15:32|      true|1694798226033799|\n",
      "+-----------+----------+---------+-----+-------+--------------+------------+-------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_customer_dim_df = spark.createDataFrame([('3', 'Sebastian', 'White', \n",
    "                    'Rome', 'IT',\n",
    "                    datetime.strptime(datetime.today().strftime('%Y-%m-%d'), '%Y-%m-%d'),\n",
    "                    datetime.strptime('2999-12-31', '%Y-%m-%d'), \n",
    "                    datetime.strptime('2020-12-09 09:15:32', '%Y-%m-%d %H:%M:%S'), True),\n",
    "                    ('2', 'Angie', 'Keller',\n",
    "                    'Paris', 'FR',\n",
    "                    datetime.strptime(datetime.today().strftime('%Y-%m-%d'), '%Y-%m-%d'),\n",
    "                    datetime.strptime('2999-12-31', '%Y-%m-%d'), \n",
    "                    datetime.strptime('2020-12-09 10:15:32', '%Y-%m-%d %H:%M:%S'), True)],\n",
    "                dim_customer_schema)\n",
    "\n",
    "new_customer_dim_df = new_customer_dim_df.withColumn(\"customer_dim_key\", random_udf())\n",
    "\n",
    "new_customer_dim_df.cache()\n",
    "\n",
    "new_customer_dim_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f00480",
   "metadata": {},
   "source": [
    "# Customers UPSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5293d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "join_cond = [customer_ice_df.customer_id == new_customer_dim_df.customer_id, customer_ice_df.is_current == True]\n",
    "\n",
    "## Find customer records to update\n",
    "customers_to_update_df = (customer_ice_df\n",
    "                          .join(new_customer_dim_df, join_cond)\n",
    "                          .select(customer_ice_df.customer_id,\n",
    "                                  customer_ice_df.first_name,\n",
    "                                  customer_ice_df.last_name,\n",
    "                                  customer_ice_df.city,\n",
    "                                  customer_ice_df.country,\n",
    "                                  customer_ice_df.eff_start_date,\n",
    "                                  new_customer_dim_df.eff_start_date.alias(\"eff_end_date\"),\n",
    "                                  customer_ice_df.customer_dim_key,\n",
    "                                  customer_ice_df.timestamp)\n",
    "                          .withColumn('is_current', lit(False))\n",
    "                         )\n",
    "\n",
    "\n",
    "## Union with new customer records\n",
    "merged_customers_df = new_customer_dim_df.unionByName(customers_to_update_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43615bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>eff_start_date</th>\n",
       "      <th>eff_end_date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_current</th>\n",
       "      <th>customer_dim_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Sebastian</td>\n",
       "      <td>White</td>\n",
       "      <td>Rome</td>\n",
       "      <td>IT</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-09 09:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694798225942495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Keller</td>\n",
       "      <td>Paris</td>\n",
       "      <td>FR</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-09 10:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694798226033799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Keller</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2020-12-08 09:15:32</td>\n",
       "      <td>False</td>\n",
       "      <td>1694797722510645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id first_name last_name     city country eff_start_date  \\\n",
       "0           3  Sebastian     White     Rome      IT     2023-09-15   \n",
       "1           2      Angie    Keller    Paris      FR     2023-09-15   \n",
       "2           2      Angie    Keller  Chicago      US     2020-10-14   \n",
       "\n",
       "  eff_end_date           timestamp  is_current  customer_dim_key  \n",
       "0   2999-12-31 2020-12-09 09:15:32        True  1694798225942495  \n",
       "1   2999-12-31 2020-12-09 10:15:32        True  1694798226033799  \n",
       "2   2023-09-15 2020-12-08 09:15:32       False  1694797722510645  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_customers_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f196584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the merged_customers_df to SQL view\n",
    "merged_customers_df.createOrReplaceTempView(\"merged_customers_view\")\n",
    "\n",
    "# Construct the MERGE INTO statement\n",
    "merge_sql = \"\"\"\n",
    "MERGE INTO glue.dip.customers AS target\n",
    "USING merged_customers_view AS source\n",
    "ON target.customer_dim_key = source.customer_dim_key\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET \n",
    "        target.first_name = source.first_name,\n",
    "        target.last_name = source.last_name,\n",
    "        target.city = source.city,\n",
    "        target.country = source.country,\n",
    "        target.eff_start_date = source.eff_start_date,\n",
    "        target.eff_end_date = source.eff_end_date,\n",
    "        target.timestamp = source.timestamp,\n",
    "        target.is_current = source.is_current\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL statement\n",
    "spark.sql(merge_sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f638588",
   "metadata": {},
   "source": [
    "# Read customers Iceberg table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21ef45a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>eff_start_date</th>\n",
       "      <th>eff_end_date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>is_current</th>\n",
       "      <th>customer_dim_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2020-09-27</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-08 09:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694797722535090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Keller</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2020-12-08 09:15:32</td>\n",
       "      <td>False</td>\n",
       "      <td>1694797722510645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Angie</td>\n",
       "      <td>Keller</td>\n",
       "      <td>Paris</td>\n",
       "      <td>FR</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-09 10:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694798226033799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sebastian</td>\n",
       "      <td>White</td>\n",
       "      <td>Rome</td>\n",
       "      <td>IT</td>\n",
       "      <td>2023-09-15</td>\n",
       "      <td>2999-12-31</td>\n",
       "      <td>2020-12-09 09:15:32</td>\n",
       "      <td>True</td>\n",
       "      <td>1694798225942495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id first_name last_name     city country eff_start_date  \\\n",
       "0           1     Morgan     Brown  Toronto  Canada     2020-09-27   \n",
       "1           2      Angie    Keller  Chicago      US     2020-10-14   \n",
       "2           2      Angie    Keller    Paris      FR     2023-09-15   \n",
       "3           3  Sebastian     White     Rome      IT     2023-09-15   \n",
       "\n",
       "  eff_end_date           timestamp  is_current  customer_dim_key  \n",
       "0   2999-12-31 2020-12-08 09:15:32        True  1694797722535090  \n",
       "1   2023-09-15 2020-12-08 09:15:32       False  1694797722510645  \n",
       "2   2999-12-31 2020-12-09 10:15:32        True  1694798226033799  \n",
       "3   2999-12-31 2020-12-09 09:15:32        True  1694798225942495  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM glue.dip.customers\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118cb8e8",
   "metadata": {},
   "source": [
    "# Add new sales for Susan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd04aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+-------------------+-----------+\n",
      "|item_id|quantity|price|          timestamp|customer_id|\n",
      "+-------+--------+-----+-------------------+-----------+\n",
      "|    103|     300| 15.8|2023-09-15 12:15:42|          2|\n",
      "|    104|      10|800.5|2023-09-15 06:35:32|          2|\n",
      "+-------+--------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_fact_df = spark.createDataFrame([('103', 300, 15.8, datetime.strptime(datetime.today().strftime('%Y-%m-%d')+' 12:15:42', '%Y-%m-%d %H:%M:%S'), '2'),\n",
    "                                       ('104', 10, 800.5, datetime.strptime(datetime.today().strftime('%Y-%m-%d')+' 06:35:32', '%Y-%m-%d %H:%M:%S'), '2')], fact_sales_schema)\n",
    "\n",
    "sales_fact_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdf1f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n",
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|          1|    Morgan|    Brown|Toronto| Canada|    2020-09-27|  2999-12-31|2020-12-08 09:15:32|      true|1694797722535090|\n",
      "|          2|     Angie|   Keller|Chicago|     US|    2020-10-14|  2999-12-31|2020-12-08 09:15:32|      true|1694797722510645|\n",
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_ice_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95369b73",
   "metadata": {},
   "source": [
    "# Reload the dataframe with latest data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20cdbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ice_df = spark.sql(\"SELECT * FROM glue.dip.customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b635e1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|customer_id|first_name|last_name|   city|country|eff_start_date|eff_end_date|          timestamp|is_current|customer_dim_key|\n",
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "|          2|     Angie|   Keller|Chicago|     US|    2020-10-14|  2023-09-15|2020-12-08 09:15:32|     false|1694797722510645|\n",
      "|          2|     Angie|   Keller|  Paris|     FR|    2023-09-15|  2999-12-31|2020-12-09 10:15:32|      true|1694798226033799|\n",
      "|          3| Sebastian|    White|   Rome|     IT|    2023-09-15|  2999-12-31|2020-12-09 09:15:32|      true|1694798225942495|\n",
      "|          1|    Morgan|    Brown|Toronto| Canada|    2020-09-27|  2999-12-31|2020-12-08 09:15:32|      true|1694797722535090|\n",
      "+-----------+----------+---------+-------+-------+--------------+------------+-------------------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_ice_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be0631",
   "metadata": {},
   "source": [
    "# Customer dimension key lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09763cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "|item_id|quantity|price|          timestamp|customer_id|customer_dim_key|\n",
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "|    103|     300| 15.8|2023-09-15 12:15:42|          2|1694798226033799|\n",
      "|    104|      10|800.5|2023-09-15 06:35:32|          2|1694798226033799|\n",
      "+-------+--------+-----+-------------------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "join_cond = [sales_fact_df.customer_id == customer_ice_df.customer_id, sales_fact_df.timestamp >= customer_ice_df.eff_start_date, sales_fact_df.timestamp <= customer_ice_df.eff_end_date]\n",
    "\n",
    "\n",
    "customers_dim_key_df = (sales_fact_df\n",
    "                          .join(customer_ice_df, join_cond, 'leftouter')\n",
    "                          .select(sales_fact_df['*'],\n",
    "                            when(customer_ice_df.customer_dim_key.isNull(), '-1').otherwise(customer_ice_df.customer_dim_key).alias(\"customer_dim_key\") )\n",
    "                         )\n",
    "\n",
    "customers_dim_key_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9085eb2",
   "metadata": {},
   "source": [
    "# Append the new sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef1f4a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customers_dim_key_df.writeTo(\"glue.dip.sales2\").append()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b944855",
   "metadata": {},
   "source": [
    "# Read the new sales table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dc04828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/docker/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>quantity</th>\n",
       "      <th>price</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_dim_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>300</td>\n",
       "      <td>15.80</td>\n",
       "      <td>2023-09-15 12:15:42</td>\n",
       "      <td>2</td>\n",
       "      <td>1694798226033799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>800.50</td>\n",
       "      <td>2023-09-15 06:35:32</td>\n",
       "      <td>2</td>\n",
       "      <td>1694798226033799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>40</td>\n",
       "      <td>90.50</td>\n",
       "      <td>2020-11-17 09:15:32</td>\n",
       "      <td>1</td>\n",
       "      <td>1694797722535090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>112</td>\n",
       "      <td>250</td>\n",
       "      <td>80.65</td>\n",
       "      <td>2020-10-28 09:15:32</td>\n",
       "      <td>1</td>\n",
       "      <td>1694797722535090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>113</td>\n",
       "      <td>10</td>\n",
       "      <td>600.50</td>\n",
       "      <td>2020-12-08 09:15:32</td>\n",
       "      <td>2</td>\n",
       "      <td>1694797722510645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  item_id  quantity   price           timestamp customer_id  customer_dim_key\n",
       "0     103       300   15.80 2023-09-15 12:15:42           2  1694798226033799\n",
       "1     104        10  800.50 2023-09-15 06:35:32           2  1694798226033799\n",
       "2     111        40   90.50 2020-11-17 09:15:32           1  1694797722535090\n",
       "3     112       250   80.65 2020-10-28 09:15:32           1  1694797722535090\n",
       "4     113        10  600.50 2020-12-08 09:15:32           2  1694797722510645"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from glue.dip.sales2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc37f0",
   "metadata": {},
   "source": [
    "# Get number of sales per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f61bde37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------+\n",
      "|country|sales_quantity|count_sales|\n",
      "+-------+--------------+-----------+\n",
      "|     US|            10|          1|\n",
      "|     FR|           310|          2|\n",
      "| Canada|           290|          2|\n",
      "+-------+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    'SELECT ct.country, SUM(st.quantity) as sales_quantity, COUNT(*) as count_sales '\n",
    "    'FROM glue.dip.sales2 st '\n",
    "    'INNER JOIN glue.dip.customers ct on st.customer_dim_key = ct.customer_dim_key group by ct.country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81bc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
